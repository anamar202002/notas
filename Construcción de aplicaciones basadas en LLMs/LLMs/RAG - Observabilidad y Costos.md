
El consumo de LLMs se mide principalmente en **tokens**, lo que introduce un modelo de costos diferente al de infraestructuras tradicionales.

La lectura enfatiza que FinOps debe integrarse desde el diseño:
- Selección del modelo adecuado para cada tarea.
- Diseño de prompts eficientes.
- Uso de caching y reutilización de respuestas.

La observabilidad permite:
- Monitorear consumo y latencia.
- Detectar anomalías.
- Ajustar dinámicamente el sistema para mantener viabilidad económica.

Un sistema RAG sin observabilidad tiende a escalar costos de forma impredecible.

Aspectos clave del modelo de costos:

- Tokens de salida suelen ser más costosos que los de entrada.
    
- Modelos más potentes son significativamente más caros.
    
- Procesamiento multimodal tiene costos adicionales.
    
- Funciones avanzadas pueden generar cargos extra.
    

### Estrategias recomendadas

- Monitoreo de tokens, latencia y errores.
    
- Alertas de consumo.
    
- Caching de respuestas.
    
- Selección dinámica de modelos.
    
- Optimización continua de prompts.
    
- Auditoría y trazabilidad.
